---
title: "W&L ch5"
author: "mht"
date: "October 16, 2014"
output: html_document
---

Chapter 5: Some examples of data analysis

5.1 Pearson correlation

Rather than just having a single number to measure the correlation, it would be nice to have a posterior distribution for r, saying how likely each possible level of correlation was. There are frequentist confidence interval methods that try to do this, as well as various analytic Bayesian results based on asymptotic approx- imations (e.g., Donner & Wells, 1986). An advantage of using a computational approach is the flexibility in the assumptions that can be made. It is possible to set up a graphical model that allows inferences about the correlation coefficient for any set of prior assumptions about the correlation.

```{r 5.1}
library(R2jags)
library(gridExtra)
library(reshape2)

setwd("/Users/mht/Documents/learning/tfbt/Lee&Wagenmakers/Code/ParameterEstimation/DataAnalysis")

x <- matrix(c(.8,102, 1,98, .5,100, 0.9,105, .7,103, 
               0.4,110, 1.2,99, 1.4,87, 0.6,113, 1.1,89, 1.3,93),
               nrow=11,ncol=2,byrow=T) 

n <- nrow(x) # number of people/units measured

data <- list("x", "n") # to be passed on to JAGS
myinits <- list(
  list(r = 0, mu = c(0,0), lambda = c(1,1)))
# parameters to be monitored:  
parameters <- c("r", "mu", "sigma")

cat('# Pearson Correlation
model{
  # Data
  for (i in 1:n){
    x[i,1:2] ~ dmnorm(mu[],TI[,])
  }
  # Priors
  mu[1] ~ dnorm(0,.001)
  mu[2] ~ dnorm(0,.001)
  lambda[1] ~ dgamma(.001,.001)
  lambda[2] ~ dgamma(.001,.001)
  r ~ dunif(-1,1)
  # Reparameterization
  sigma[1] <- 1/sqrt(lambda[1])
  sigma[2] <- 1/sqrt(lambda[2])
  T[1,1] <- 1/lambda[1]
  T[1,2] <- r*sigma[1]*sigma[2]
  T[2,1] <- r*sigma[1]*sigma[2]
  T[2,2] <- 1/lambda[2]
  TI[1:2,1:2] <- inverse(T[1:2,1:2])
}', file={f<-tempfile()})

# The following command calls JAGS with specific options.
# For a detailed description see the R2jags documentation.
samples = jags(data, inits=myinits, parameters,
   			 model.file =f, n.chains=1, n.iter=5000, 
         n.burnin=1, n.thin=1, DIC=T)

# Now the values for the monitored parameters are in the "samples" object, 
# ready for inspection.

dr <- data.frame(r=samples$BUGSoutput$sims.list$r)
dx <- data.frame(x)
#Frequentist point-estimate of r:
freq.r <- with(dx,cor(X1,X2))

a<-qplot(data=dx,x=X1,y=X2,geom='point')+theme_bw()+
  xlab('response time (sec)')+
  ylab('IQ')
b<-ggplot(data=dr,aes(x=r))+
  geom_density()+
  geom_vline(xintercept = freq.r, linetype='longdash')+
  theme_bw()+
  xlab('correlation')+
  xlim(-1,0.5)

#quartz('correlation, dataset 1')
grid.arrange(a,b,nrow=1)
```


Exercise 5.1.1 The second data set in the Matlab and R code is just the first data set from Figure 5.2 repeated twice. Set dataset=2 to consider these repeated data, and interpret the differences in the posterior distributions for r.

```{r 5.1.1}
x <- matrix(c(.8,102, 1,98, .5,100, 0.9,105, .7,103, 
             0.4,110, 1.2,99, 1.4,87, 0.6,113, 1.1,89, 1.3,93,
             .8,102, 1,98, .5,100, 0.9,105, .7,103, 
             0.4,110, 1.2,99, 1.4,87, 0.6,113, 1.1,89, 1.3,93),
             nrow=22,ncol=2,byrow=T) 

n <- nrow(x) # number of people/units measured

data <- list("x", "n") # to be passed on to JAGS
myinits <- list(
  list(r = 0, mu = c(0,0), lambda = c(1,1)))
# parameters to be monitored:  
parameters <- c("r", "mu", "sigma")

# The following command calls JAGS with specific options.
# For a detailed description see the R2jags documentation.
samples = jags(data, inits=myinits, parameters,
   			 model.file =f, n.chains=1, n.iter=5000, 
         n.burnin=1, n.thin=1, DIC=T)
# Now the values for the monitored parameters are in the "samples" object, 
# ready for inspection.

dr <- data.frame(r=samples$BUGSoutput$sims.list$r)
dx <- data.frame(x)
#Frequentist point-estimate of r:
freq.r <- with(dx,cor(X1,X2))

c<-qplot(data=dx,x=X1,y=X2,geom='point')+theme_bw()+
  xlab('response time (sec)')+
  ylab('IQ')
d<-ggplot(data=dr,aes(x=r))+
  geom_density()+
  geom_vline(xintercept = freq.r, linetype='longdash')+
  theme_bw()+
  xlab('correlation')+
  xlim(-1,0.5)

#quartz('correlation, dataset 2')
grid.arrange(c,d,nrow=1)
```

Exercise 5.1.2 Do you find the priors on μ1 and μ2 to be reasonable?

The priors on the means are gaussians with low precision. The mean for each is 0, however the means for these two measures are quite different (IQ _ mean ~ 100; RT _ mean ~ 1). Perhaps this is worth changing. Also, we could import our knowledge of the IQ test into the prior for IQ scores.

Let's try that.

```{r 5.1.2}
x <- matrix(c(.8,102, 1,98, .5,100, 0.9,105, .7,103, 
               0.4,110, 1.2,99, 1.4,87, 0.6,113, 1.1,89, 1.3,93),
               nrow=11,ncol=2,byrow=T) 

n <- nrow(x) # number of people/units measured

data <- list("x", "n") # to be passed on to JAGS
myinits <- list(
  list(r = 0, mu = c(0,0), lambda = c(1,1)))
# parameters to be monitored:  
parameters <- c("r", "mu", "sigma")

cat('# Pearson Correlation
model{
  # Data
  for (i in 1:n){
    x[i,1:2] ~ dmnorm(mu[],TI[,])
  }
  # Priors
  mu[1] ~ dnorm(1,.001)
  mu[2] ~ dnorm(100,.0044)
  lambda[1] ~ dgamma(.001,.001)
  lambda[2] ~ dgamma(.001,.001)
  r ~ dunif(-1,1)
  # Reparameterization
  sigma[1] <- 1/sqrt(lambda[1])
  sigma[2] <- 1/sqrt(lambda[2])
  T[1,1] <- 1/lambda[1]
  T[1,2] <- r*sigma[1]*sigma[2]
  T[2,1] <- r*sigma[1]*sigma[2]
  T[2,2] <- 1/lambda[2]
  TI[1:2,1:2] <- inverse(T[1:2,1:2])
}', file = {g <- tempfile()})

# The following command calls JAGS with specific options.
# For a detailed description see the R2jags documentation.
samples = jags(data, inits=myinits, parameters,
   			 model.file =g, n.chains=1, n.iter=5000, 
         n.burnin=1, n.thin=1, DIC=T)
# Now the values for the monitored parameters are in the "samples" object, 
# ready for inspection.

dr <- data.frame(r=samples$BUGSoutput$sims.list$r)
dx <- data.frame(x)
#Frequentist point-estimate of r:
freq.r <- with(dx,cor(X1,X2))

a<-qplot(data=dx,x=X1,y=X2,geom='point')+theme_bw()+
  xlab('response time (sec)')+
  ylab('IQ')
b<-ggplot(data=dr,aes(x=r))+
  geom_density()+
  geom_vline(xintercept = freq.r, linetype='longdash')+
  theme_bw()+
  xlab('correlation')+
  xlim(-1,0.5)

#quartz('correlation, dataset 1, alt priors')
grid.arrange(a,b,nrow=1)
```

Exercise 5.1.3 The current graphical model assumes that the values from the two variables—the xi = (xi1, xi2)—are observed with perfect accuracy. When might this be a problematic assumption? How could the current approach be extended to make more realistic assumptions?

Ans: measurement error in tests... IQ tests have some noise, RT data has some noise. But is this already captured by sigma?

### 5.2 Person correlation with uncertainty

The observed data still take the form xi = (xi1,xi2) for the ith person’s response time and IQ measure. But these observations are now sampled from a Gaussian distribution, centered on the unobserved true response time and IQ of that person, denoted yi = (yi1, yi2). These true values are then modeled as the x were in the previous model in Figure 5.1, as draws from a multivariate Gaussian distribution.


```{r 5.2.1}
x <- matrix(c(.8,102, 1,98, .5,100, 0.9,105, .7,103, 
             0.4,110, 1.2,99, 1.4,87, 0.6,113, 1.1,89, 1.3,93),
             nrow=11,ncol=2,byrow=T) 
n <- nrow(x) # number of people/units measured

#precision of measurement:
sigmaerror <- c(.03, 1) # both measurements quite precise
# sigmaerror = c(.03, 10)
lambdaerror <- 1/sigmaerror^2

data <- list("x", "n", "lambdaerror") # to be passed on to JAGS
myinits <-  list(
  list(r = 0, mu = c(0,0), lambda = c(1,1)))
# parameters to be monitored:	
parameters <- c("r", "mu", "sigma")

cat('# Pearson Correlation With Uncertainty in Measurement
model{
  # Data
  for (i in 1:n){
    y[i,1:2] ~ dmnorm(mu[],TI[,])
    for (j in 1:2){
      x[i,j] ~ dnorm(y[i,j],lambdaerror[j])
    }
  }
  # Priors
  mu[1] ~ dnorm(0,.001)
  mu[2] ~ dnorm(0,.001)
  lambda[1] ~ dgamma(.001,.001)
  lambda[2] ~ dgamma(.001,.001)
  r ~ dunif(-1,1)
  # Reparameterization
  sigma[1] <- 1/sqrt(lambda[1])
  sigma[2] <- 1/sqrt(lambda[2])
  T[1,1] <- 1/lambda[1]
  T[1,2] <- r*sigma[1]*sigma[2]
  T[2,1] <- r*sigma[1]*sigma[2]
  T[2,2] <- 1/lambda[2]
  TI[1:2,1:2] <- inverse(T[1:2,1:2])
}', file= {correlation_2<- tempfile()})


# The following command calls JAGS with specific options.
# For a detailed description see the R2jags documentation.
samples <- jags(data, inits=myinits, parameters,
	 			 model.file = correlation_2, n.chains=1, n.iter=5000, 
         n.burnin=1, n.thin=1, DIC=T)
# Now the values for the monitored parameters are in the "samples" object, 
# ready for inspection.

dr <- data.frame(r=samples$BUGSoutput$sims.list$r)

dx <- data.frame(x)
#Frequentist point-estimate of r:
freq.r <- with(dx,cor(X1,X2))
limits <- aes(xmin=, xmax =,
              ymax = resp + se, ymin=resp - se)

a<-ggplot(data=dx,aes(x=X1,y=X2))+
  geom_point()+
  geom_errorbarh(aes(xmin=X1-sigmaerror[1],xmax=X1+sigmaerror[1]),height=0.001)+
  geom_errorbar(aes(ymin=X2-sigmaerror[2],ymax=X2+sigmaerror[2]),width=0.001)+
  theme_bw()+
  xlab('response time (sec)')+
  ylab('IQ')


b<-ggplot(data=dr,aes(x=r))+
  geom_density()+
  geom_vline(xintercept = freq.r, linetype='longdash')+
  theme_bw()+
  xlab('correlation')+
  xlim(-1,0.5)

#quartz('correlation with uncertainty, dataset 1')
grid.arrange(a,b,nrow=1)
```

Exercise 5.2.2 Generate results for the second data set, which changes σ = 10 for the IQ measurement. Compare these results with those obtained assuming σ = 1 .

```{r 5.2.2}
x <- matrix(c(.8,102, 1,98, .5,100, 0.9,105, .7,103, 
             0.4,110, 1.2,99, 1.4,87, 0.6,113, 1.1,89, 1.3,93),
             nrow=11,ncol=2,byrow=T) 
n <- nrow(x) # number of people/units measured

#precision of measurement:
#sigmaerror <- c(.03, 1) # both measurements quite precise
sigmaerror = c(.03, 10)
lambdaerror <- 1/sigmaerror^2

data <- list("x", "n", "lambdaerror") # to be passed on to JAGS
myinits <-  list(
  list(r = 0, mu = c(0,0), lambda = c(1,1)))
# parameters to be monitored:  
parameters <- c("r", "mu", "sigma")

# The following command calls JAGS with specific options.
# For a detailed description see the R2jags documentation.
samples <- jags(data, inits=myinits, parameters,
	 			 model.file = correlation_2, n.chains=1, n.iter=5000, 
         n.burnin=1, n.thin=1, DIC=T)
# Now the values for the monitored parameters are in the "samples" object, 
# ready for inspection.

dr <- data.frame(r=samples$BUGSoutput$sims.list$r)

dx <- data.frame(x)
#Frequentist point-estimate of r:
freq.r <- with(dx,cor(X1,X2))
limits <- aes(xmin=, xmax =,
              ymax = resp + se, ymin=resp - se)

a<-ggplot(data=dx,aes(x=X1,y=X2))+
  geom_point()+
  geom_errorbarh(aes(xmin=X1-sigmaerror[1],xmax=X1+sigmaerror[1]),height=0.001)+
  geom_errorbar(aes(ymin=X2-sigmaerror[2],ymax=X2+sigmaerror[2]),width=0.001)+
  theme_bw()+
  xlab('response time (sec)')+
  ylab('IQ')


b<-ggplot(data=dr,aes(x=r))+
  geom_density()+
  geom_vline(xintercept = freq.r, linetype='longdash')+
  theme_bw()+
  xlab('correlation')+
  xlim(-1,0.5)

#quartz('correlation with more uncertainty on IQ, dataset 1')
grid.arrange(a,b,nrow=1)
```

Exercise 5.2.3 The graphical model in Figure 5.3 assumes the uncertainty for each variable is known. How could this assumption be relaxed to the case where the uncertainty is unknown?

PRIORS

Exercise 5.2.4 The graphical model in Figure 5.3 assumes the uncertainty for each variable is the same for all observations. How could this assumption be relaxed to the case where, for example, extreme IQs are less accurately measured than IQs in the middle of the standard distribution?

PRIORS on a subject level?
